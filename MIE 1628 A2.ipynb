{"cells":[{"cell_type":"code","source":["# A1\n\nfrom pyspark import SparkContext\nsc = SparkContext.getOrCreate()\n\ndata = sc.textFile(\"dbfs:/FileStore/MIE_1628_A2/integer.txt\")\n\nint_data = data.map(lambda x: int(x))\n\neven_numbers = int_data.filter(lambda x: x % 2 == 0).count()\nodd_numbers = int_data.filter(lambda x: x % 2 == 1).count()\n\nprint(f'even_numbers={even_numbers}')\nprint(f'odd_numbers={odd_numbers}')"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{"rowLimit":10000,"byteLimit":2048000},"nuid":"db4d0df1-4270-4458-abf3-41819eeb1da5","inputWidgets":{},"title":""}},"outputs":[{"output_type":"stream","output_type":"stream","name":"stdout","text":["even_numbers=514\nodd_numbers=496\n"]}],"execution_count":0},{"cell_type":"code","source":["# A2\n\ndata = sc.textFile(\"dbfs:/FileStore/MIE_1628_A2/salary.txt\")\nres = data.map(lambda line: (line.split(' ')[0], int(line.split(' ')[1]))).reduceByKey(lambda x,y: x + y)\nprint(f'salary_sum={res.collect()}')"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{"rowLimit":10000,"byteLimit":2048000},"nuid":"c409112d-3d5d-4b69-b5cf-4cbb7c3c4952","inputWidgets":{},"title":""}},"outputs":[{"output_type":"stream","output_type":"stream","name":"stdout","text":["salary_sum=[('Sales', 3488491), ('Research', 3328284), ('Developer', 3221394), ('QA', 3360624), ('Marketing', 3158450)]\n"]}],"execution_count":0},{"cell_type":"code","source":["# A3\nfrom operator import add\nkeywords = ['Shakespeare', 'why', 'Lord', 'Library', 'GUTENBERG', 'WILLIAM', 'COLLEGE', 'WORLD']\n\nrdd = sc.textFile(\"dbfs:/FileStore/MIE_1628_A2/shakespeare_1.txt\")\nwords = rdd.flatMap(lambda line: line.split(' '))\n\nkeyword_rdd = words.filter(lambda word : word in keywords)\n\nkeyword_pairs = keyword_rdd.map(lambda keyword: (keyword, 1))\ncounts = keyword_pairs.reduceByKey(add)\n\nprint(f'count of keywords={counts.collect()}')"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{"rowLimit":10000,"byteLimit":2048000},"nuid":"1f0094eb-9a40-4cae-bcd1-7c7b293928fa","inputWidgets":{},"title":""}},"outputs":[{"output_type":"stream","output_type":"stream","name":"stdout","text":["count of keywords=[('Shakespeare', 22), ('GUTENBERG', 99), ('WILLIAM', 115), ('WORLD', 98), ('COLLEGE', 98), ('why', 91), ('Lord', 341), ('Library', 2)]\n"]}],"execution_count":0},{"cell_type":"code","source":["# A4\nrdd = sc.textFile(\"dbfs:/FileStore/MIE_1628_A2/shakespeare_1.txt\")\nwords = rdd.flatMap(lambda line: line.split(' ')).flatMap(lambda line: line.split(',')).flatMap(lambda line: line.split(':')).flatMap(lambda line: line.split('*')).filter(lambda word : len(word) > 0)\nkey_values = words.map(lambda word: (word, 1))\ncounts = key_values.reduceByKey(add)\n\n\ndf = counts.toDF([\"word\", \"count\"])\ndf.createOrReplaceTempView(\"word_counts\")\n\ntop_15 = spark.sql(\"SELECT * FROM word_counts ORDER BY count DESC LIMIT 15\")\nprint(\"top 15 words:\")\ntop_15.show()\n\nbottom_15 = spark.sql(\"SELECT * FROM word_counts ORDER BY count ASC LIMIT 15\")\nprint(\"bottom 15 word:\")\nbottom_15.show()"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{"rowLimit":10000,"byteLimit":2048000},"nuid":"11ed3351-e8e6-4b00-bcc3-656250f3035f","inputWidgets":{},"title":""}},"outputs":[{"output_type":"stream","output_type":"stream","name":"stdout","text":["top 15 words:\n+----+-----+\n|word|count|\n+----+-----+\n| the|11397|\n| and| 8904|\n|   I| 8706|\n|  of| 7888|\n|  to| 7453|\n|   a| 5673|\n|  my| 4914|\n| you| 4683|\n|  in| 4642|\n| And| 3732|\n|that| 3636|\n|  is| 3584|\n| not| 3347|\n| his| 3242|\n|with| 3183|\n+----+-----+\n\nbottom 15 word:\n+--------------+-----+\n|          word|count|\n+--------------+-----+\n|         START|    1|\n|          2011|    1|\n|     cooperate|    1|\n|       NEITHER|    1|\n|         read!|    1|\n|       January|    1|\n|       License|    1|\n|        anyone|    1|\n|         Title|    1|\n|          1994|    1|\n|         EBOOK|    1|\n|  restrictions|    1|\n|     September|    1|\n|     Character|    1|\n|WORKS--WILLIAM|    1|\n+--------------+-----+\n\n"]}],"execution_count":0},{"cell_type":"code","source":["# B1\ndf = spark.read.csv(\"dbfs:/FileStore/MIE_1628_A2/movies.csv\", inferSchema=True, header=True)\ndf.createOrReplaceTempView(\"movies\")\ntop_20_movies = spark.sql(\"\"\"\n    SELECT movieId, AVG(rating) as avg_rating \n    FROM movies \n    GROUP BY movieId \n    ORDER BY avg_rating DESC \n    LIMIT 20\"\"\")\ntop_20_movies.show()\n\ntop_15_users = spark.sql(\"\"\"\n    SELECT userId, AVG(rating)\n    FROM movies\n    GROUP BY userId\n    ORDER BY avg(rating) DESC\n    LIMIT 15\"\"\")\ntop_15_users.show()"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{"rowLimit":10000,"byteLimit":2048000},"nuid":"6fdab8d0-e533-4aaa-8fb4-5a0de4037486","inputWidgets":{},"title":""}},"outputs":[{"output_type":"stream","output_type":"stream","name":"stdout","text":["+-------+------------------+\n|movieId|        avg_rating|\n+-------+------------------+\n|     32|2.9166666666666665|\n|     90|            2.8125|\n|     30|               2.5|\n|     94| 2.473684210526316|\n|     23| 2.466666666666667|\n|     49|            2.4375|\n|     29|               2.4|\n|     18|               2.4|\n|     52| 2.357142857142857|\n|     53|              2.25|\n|     62|              2.25|\n|     92|2.2142857142857144|\n|     46|               2.2|\n|     68|2.1578947368421053|\n|     87|2.1333333333333333|\n|      2|2.1052631578947367|\n|     69| 2.076923076923077|\n|     27| 2.066666666666667|\n|     88|2.0555555555555554|\n|     22|              2.05|\n+-------+------------------+\n\n+------+------------------+\n|userId|       avg(rating)|\n+------+------------------+\n|    11|2.2857142857142856|\n|    26| 2.204081632653061|\n|    22|2.1607142857142856|\n|    23|2.1346153846153846|\n|     2|2.0652173913043477|\n|    17|1.9565217391304348|\n|     8|1.8979591836734695|\n|    24|1.8846153846153846|\n|    12|1.8545454545454545|\n|     3|1.8333333333333333|\n|    29| 1.826086956521739|\n|    28|              1.82|\n|     9|1.7924528301886793|\n|    14|1.7894736842105263|\n|    16|1.7777777777777777|\n+------+------------------+\n\n"]}],"execution_count":0},{"cell_type":"code","source":["# B2, B3\nfrom pyspark.ml.recommendation import ALS\nfrom pyspark.ml.evaluation import RegressionEvaluator\n\ndf = spark.read.csv(\"dbfs:/FileStore/MIE_1628_A2/movies.csv\", inferSchema=True, header=True)\n(train, test) = df.randomSplit([0.8, 0.2])\n\nals = ALS(userCol='userId', itemCol='movieId', ratingCol='rating', coldStartStrategy='drop')\n\nmodel = als.fit(train)\nprediction = model.transform(test)\n\nrmse_evaluator = RegressionEvaluator(\n    metricName='rmse',\n    labelCol='rating',\n    predictionCol='prediction'\n)\n\nmae_evaluator = RegressionEvaluator(\n    metricName='mae',\n    labelCol='rating',\n    predictionCol='prediction'\n)\n\nrmse = rmse_evaluator.evaluate(prediction)\nmae = mae_evaluator.evaluate(prediction)\nprint(\"root mean square error for [80, 20] split=\" + str(rmse))\nprint(\"mean absolute error for [80, 20] split=\" + str(mae))\nprint(\"prediction for [80, 20] split:\")\nprediction.show()\n\n\n(train, test) = df.randomSplit([0.6, 0.4])\nmodel = als.fit(train)\nprediction = model.transform(test)\nrmse = rmse_evaluator.evaluate(prediction)\nmae = mae_evaluator.evaluate(prediction)\nprint(\"root mean square error for [60, 40] split=\" + str(rmse))\nprint(\"mean absolute error for [60, 40] split=\" + str(mae))\nprint(\"prediction for [60, 40] split:\")\nprediction.show()"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{"rowLimit":10000,"byteLimit":2048000},"nuid":"12902f30-1da0-4bf8-ae0f-588e9a4d42cf","inputWidgets":{},"title":""}},"outputs":[{"output_type":"stream","output_type":"stream","name":"stdout","text":["root mean square error for [80, 20] split=0.9777890272123972\nmean absolute error for [80, 20] split=0.6860865863743207\nprediction for [80, 20] split:\n+-------+------+------+----------+\n|movieId|rating|userId|prediction|\n+-------+------+------+----------+\n|      1|     1|    28| 0.9467137|\n|     19|     3|    28| 2.2585254|\n|     36|     1|    28| 1.4560488|\n|     38|     2|    28| 1.6508074|\n|     50|     1|    28| 1.3009136|\n|     54|     1|    28| 0.9556073|\n|     62|     3|    28| 1.2522787|\n|     65|     1|    28|0.74010944|\n|     88|     2|    28| 1.3611788|\n|     98|     1|    28| 1.2618763|\n|     99|     1|    28| 1.1259569|\n|      1|     1|    26|  1.637441|\n|      3|     1|    26| 0.8077318|\n|      7|     5|    26| 1.6972333|\n|      9|     1|    27| 0.6673592|\n|     13|     3|    26| 1.6228696|\n|     16|     1|    26| 1.2412927|\n|     24|     5|    26| 2.4115663|\n|     33|     3|    27| 1.1142951|\n|     44|     1|    26| 1.2683893|\n+-------+------+------+----------+\nonly showing top 20 rows\n\nroot mean square error for [60, 40] split=1.232438987873359\nmean absolute error for [60, 40] split=0.8396543271090938\nprediction for [60, 40] split:\n+-------+------+------+----------+\n|movieId|rating|userId|prediction|\n+-------+------+------+----------+\n|      0|     3|    28| 1.8592505|\n|      1|     1|    28| 0.6332451|\n|      3|     1|    28|0.84007865|\n|     14|     1|    28|0.88952196|\n|     17|     1|    28| 2.2424462|\n|     19|     3|    28| 1.0858397|\n|     24|     3|    28|0.70609295|\n|     27|     1|    28| 0.5402754|\n|     36|     1|    28| 1.5526905|\n|     39|     2|    28| 1.3174429|\n|     45|     1|    28| 1.6678362|\n|     54|     1|    28| 1.3517838|\n|     62|     3|    28| 1.2910267|\n|     65|     1|    28|  2.937756|\n|     75|     1|    28| 0.6428861|\n|     78|     1|    28| 0.9109102|\n|     81|     5|    28| 1.1656811|\n|     83|     1|    28| 2.1767857|\n|     88|     2|    28| 1.0034508|\n|     89|     4|    28| 2.2003422|\n+-------+------+------+----------+\nonly showing top 20 rows\n\n"]}],"execution_count":0},{"cell_type":"code","source":["#B3\nfrom pyspark.ml.tuning import ParamGridBuilder, TrainValidationSplit, CrossValidator\n(train, test) = df.randomSplit([0.8, 0.2])\n\nparameters=ParamGridBuilder()\\\n    .addGrid(als.rank,[10, 50, 100])\\\n    .addGrid(als.maxIter, [10, 15, 20])\\\n    .addGrid(als.regParam, [0.01, 0.05, 0.1])\\\n    .build()\n\neval = RegressionEvaluator(metricName='rmse', labelCol='rating', predictionCol='prediction')\n\ntrainvs = TrainValidationSplit(estimator=als, estimatorParamMaps=parameters, evaluator=eval)\ncv = CrossValidator(estimator=als, estimatorParamMaps=parameters, evaluator=eval, numFolds=3)\n\nmodel = trainvs.fit(train)\npredictions = model.transform(test)\nrmse = eval.evaluate(predictions)\n\nprint(\"best ranks=\", model.bestModel._java_obj.parent().getRank())\nprint(\"best max iter=\", model.bestModel._java_obj.parent().getMaxIter())\nprint(\"best regParam=\", model.bestModel._java_obj.parent().getRegParam())\nprint(\"rmse=\", rmse)\n\n"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{"rowLimit":10000,"byteLimit":2048000},"nuid":"6f53aa97-d655-4b19-b92b-649ea29a8aef","inputWidgets":{},"title":""}},"outputs":[{"output_type":"stream","output_type":"stream","name":"stdout","text":["best ranks= 50\nbest max iter= 20\nbest regParam= 0.1\nrmse= 0.9681408715390586\n"]}],"execution_count":0},{"cell_type":"code","source":["# B5\n\nbest_model = model.bestModel\nuser_df = spark.createDataFrame([(10,), (14,)], ['userId'])\nrecommendations = best_model.recommendForUserSubset(user_df, 15)\nrecommendations.show(truncate=False)\n\n"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{"rowLimit":10000,"byteLimit":2048000},"nuid":"fc987601-85b9-4753-901f-da1ec41e55f0","inputWidgets":{},"title":""}},"outputs":[{"output_type":"stream","output_type":"stream","name":"stdout","text":["+------+--------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n|userId|recommendations                                                                                                                                                                                                                                         |\n+------+--------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n|10    |[{40, 3.3974857}, {92, 3.3556578}, {2, 3.0700889}, {89, 2.9030402}, {12, 2.8274043}, {62, 2.727406}, {25, 2.7179892}, {49, 2.647411}, {42, 2.504928}, {39, 2.3664346}, {4, 2.2915287}, {82, 2.2527263}, {0, 2.2284079}, {93, 2.202732}, {95, 2.05912}]  |\n|14    |[{52, 4.4258785}, {29, 4.38502}, {63, 4.175415}, {96, 3.4437637}, {62, 3.394372}, {43, 3.3396666}, {72, 3.3395965}, {85, 3.2726846}, {53, 3.0610538}, {69, 2.8776789}, {58, 2.723449}, {70, 2.6479058}, {2, 2.6218104}, {67, 2.606774}, {14, 2.5685108}]|\n+------+--------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n\n"]}],"execution_count":0}],"metadata":{"application/vnd.databricks.v1+notebook":{"notebookName":"MIE 1628 A2","dashboards":[],"notebookMetadata":{"pythonIndentUnit":4},"language":"python","widgets":{}}},"nbformat":4,"nbformat_minor":0}
